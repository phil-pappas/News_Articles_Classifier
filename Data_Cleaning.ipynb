{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jt -t chesterish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To change the theme use the following command:\n",
    "!jt -t <theme-name>\n",
    "\n",
    "Theme names: \n",
    "1) chesterish \n",
    "2) onedork  (currently using)\n",
    "    !jt -t onedork -f roboto -fs 12\n",
    "3) monokai\n",
    " more here: https://github.com/dunovank/jupyter-themes\n",
    " \n",
    " \n",
    " other: https://towardsdatascience.com/bringing-the-best-out-of-jupyter-notebooks-for-data-science-f0871519ca29\n",
    "'''"
   ]
  },
  {
   "source": [
    "Now that we have gathered all the data, its time to clean and prepare them for our clustering algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "%run packages_imported.py\n",
    "%run EDA.py\n",
    "%run NLP.py\n",
    "%run Cleaner.py"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "source": [
    "Load the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' FOR SMALLER DF - NEEDS TO GO AT THE BEGINNING\n",
    "frame1 = df_all_news_processed.loc[df_all_news_processed['Category'] == 'entertainment'][:1200]\n",
    "frame2 = df_all_news_processed.loc[df_all_news_processed['Category'] == 'financial'][:1600]\n",
    "frame3 = df_all_news_processed.loc[df_all_news_processed['Category'] == 'political'][:2000]\n",
    "frame4 = df_all_news_processed.loc[df_all_news_processed['Category'] == 'sport'][:4500]\n",
    "frame5 = df_all_news_processed.loc[df_all_news_processed['Category'] == 'technology'][:2800]\n",
    "frame6 = df_all_news_processed.loc[df_all_news_processed['Category'] == 'world'][:1000]\n",
    "frame7 = df_all_news_processed.loc[df_all_news_processed['Category'] == 'travel'][:3800]\n",
    "\n",
    "frames = [frame1, frame2, frame3, frame4, frame5, frame6, frame7]\n",
    "df_processed_smaller = pd.concat(frames)\n",
    "df_processed_smaller.reset_index()\n",
    "df_processed_smaller.to_pickle('data/original_data/pickles/df_processed_smaller_before_NLP')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all_news = pd.read_excel(\"data\\\\original_data\\\\all_news.xlsx\")\n",
    "#df_all_news.drop(['Unnamed: 0'], axis=1, inplace=True) #drop the unecessary columns\n",
    "\n",
    "df_all_news = pd.read_pickle(r'data/original_data/pickles/all_news')\n",
    "\n",
    "#store the number of records. Will be used for generating statistics\n",
    "initial_size = len(df_all_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_news.head()"
   ]
  },
  {
   "source": [
    "## Exploring the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Count the number of records for each category and store it in a secondary dataframe that it will help us produce our visualisations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process_stats = EDA.count_records_per_label(df_all_news, \"Count_Pre_Processing\")"
   ]
  },
  {
   "source": [
    "EDA.display_distribution_donut(df_process_stats, 'Count_Pre_Processing', 'Category')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "We can clearly see that the sports class is dominating as it's sometimes x3 or almost x4 times bigger that the rest. Class political that it comes second, also seems to be double the size of the smaller classes. That is a hint that it tells us that most probably we are going to need to balance our classess before proceed to the modeling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "As always, let's check for null values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all_news = Cleaner.drop_null_values(df_all_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all_news = Cleaner.drop_null_values(df_all_news)\n",
    "df_all_news.isnull().any()"
   ]
  },
  {
   "source": [
    "#### OK, so there no null values, but what about duplicated records? Since we are talking about news articles with hundrends of words per article, we assume that there are no identical articles. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_news.Text.duplicated().any()"
   ]
  },
  {
   "source": [
    "It appears that there are duplicated records. But that's weird for text articles, isn't it? Curious how they look like? Let's give a look"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary = df_all_news.loc[df_all_news.Text.duplicated()]\n",
    "temporary.sort_values(by='Text', ascending=False, inplace=True)\n",
    "temporary"
   ]
  },
  {
   "source": [
    "According to duplicated retrieval function there are more than 100k duplicated records (!), that is 1/5 of our initial dataset. <br/>However we see from the head() of the dataset that are a lot of non-ascii characters in the beginning of the articles and even though they have different content, they are recognised as incorrectly as duplicates. <br/>One other interesting thing that we observed is that in the end of the dataframe, '\\n' is considered to be the smallest 'word'. That explains why we didn't find any null values in the dataframe.<br/><br/>In order to take a deeper look into this, we will count the number of words of each article but first let's free some memory ;) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del [temporary]\n",
    "gc.collect()"
   ]
  },
  {
   "source": [
    "#### Count the number of words of each article - Time comparisons between two options"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time() \n",
    "df_all_news['Text_TotalWords'] = EDA.count_words_per_records_opt_1(df_all_news)\n",
    "EDA.process_time(round(time.time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Process completed.\nTime taken: 16 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time() \n",
    "#df.col.apply(lambda x: len(x.split()))\n",
    "df_all_news['Text_TotalWords'] = EDA.count_words_per_records_opt_2(df_all_news)\n",
    "EDA.process_time(round(time.time()-t1))"
   ]
  },
  {
   "source": [
    "TODO: comment about the two comparisons"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Removed 16195 records with less than 10 words\n"
     ]
    }
   ],
   "source": [
    "df_all_news.reset_index(drop=True, inplace=True)\n",
    "df_all_news_processed = df_all_news.copy()\n",
    "\n",
    "#replaces the '\\n' with single space\n",
    "df_all_news_processed = Cleaner.remove_new_line(df_all_news_processed)\n",
    "\n",
    "#explain why we remove 10 and less. perhaps an article to answer this?\n",
    "df_all_news_processed = df_all_news_processed[df_all_news_processed['Text_TotalWords'] > 10]  \n",
    "print(\"Removed {} records with less than 10 words\".format((len(df_all_news)-len(df_all_news_processed))))\n",
    "\n",
    "#df_all_news_processed.to_pickle('data/original_data/pickles/df_all_news_processed_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        Category  Count_Pre_Processing  Count_Post_Processing\n",
       "0  entertainment                 50282                  47279\n",
       "1      financial                 47851                  47271\n",
       "2      political                 87157                  85315\n",
       "3          sport                156899                 154349\n",
       "4     technology                 41476                  39804\n",
       "5         travel                 49470                  43832\n",
       "6          world                 60297                  59387"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Category</th>\n      <th>Count_Pre_Processing</th>\n      <th>Count_Post_Processing</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>entertainment</td>\n      <td>50282</td>\n      <td>47279</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>financial</td>\n      <td>47851</td>\n      <td>47271</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>political</td>\n      <td>87157</td>\n      <td>85315</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>sport</td>\n      <td>156899</td>\n      <td>154349</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>technology</td>\n      <td>41476</td>\n      <td>39804</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>travel</td>\n      <td>49470</td>\n      <td>43832</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>world</td>\n      <td>60297</td>\n      <td>59387</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df_process_stats['Count_Post_Processing'] = EDA.count_records_per_label(df_all_news_processed, \"Count_Post_Processing\")['Count_Post_Processing']\n",
    "df_process_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total records pre-processing: 493432\nTotal subtracted records: -16195\nTotal records post-processing: 477237\nTotal loss: -3.28% of the initial records\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        Category  Count_Pre_Processing  Count_Post_Processing Records_Loss\n",
       "0  entertainment                 50282                  47279       -5.97%\n",
       "1      financial                 47851                  47271       -1.21%\n",
       "2      political                 87157                  85315       -2.11%\n",
       "3          sport                156899                 154349       -1.63%\n",
       "4     technology                 41476                  39804       -4.03%\n",
       "5         travel                 49470                  43832       -11.4%\n",
       "6          world                 60297                  59387       -1.51%"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Category</th>\n      <th>Count_Pre_Processing</th>\n      <th>Count_Post_Processing</th>\n      <th>Records_Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>entertainment</td>\n      <td>50282</td>\n      <td>47279</td>\n      <td>-5.97%</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>financial</td>\n      <td>47851</td>\n      <td>47271</td>\n      <td>-1.21%</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>political</td>\n      <td>87157</td>\n      <td>85315</td>\n      <td>-2.11%</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>sport</td>\n      <td>156899</td>\n      <td>154349</td>\n      <td>-1.63%</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>technology</td>\n      <td>41476</td>\n      <td>39804</td>\n      <td>-4.03%</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>travel</td>\n      <td>49470</td>\n      <td>43832</td>\n      <td>-11.4%</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>world</td>\n      <td>60297</td>\n      <td>59387</td>\n      <td>-1.51%</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "#calculate the difference in percentages. Note: the above cell is mandatory for this execution\n",
    "df_process_stats = EDA.calculate_loss_percentages(df_process_stats, len(df_all_news_processed), initial_size)\n",
    "df_process_stats"
   ]
  },
  {
   "source": [
    "EDA.display_stats_distribution_pre_post_processing(df_process_stats,'Removing articles with less than 10 words (hover for percentages)')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "As we see, there were not significant changes between the categories until now. However, we see that the classes are quite unbalanced between them. We will keep this in mind for now and will review it after the processing.<br/>\n",
    "*Tip: Click on Pre-Processing to get a view on the current balance of the dataset only*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#delete future unused variables at this point\n",
    "del df_all_news, index, row, temp, lossPercentage, df_process_stats, total_loss\n",
    "gc.collect()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_all_news_processed = pd.read_pickle(r'data/original_data/pickles/df_all_news_processed_v1')\n",
    "#df_all_news_processed"
   ]
  },
  {
   "source": [
    "### Discovering outliers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.display_outliers_boxplot(df_all_news_processed, \"Text_TotalWords\", \"Text_TotalWords\", None, \"Visualise the article outliers with extreme number of words (zoom in)\")"
   ]
  },
  {
   "source": [
    "Using a boxplot to visualise outliers help us understand better the normal distribution of the dataset. However here, due to extreme outliers need to zoom in in order to view the quantiles of the boxplot. After zooming in, we see that the q4 quantile is laying between 1200-1300 total words, for simplicity reasons let's hypothesize 1400 for now. Extreme outliers such as above 1500 number of wrods, are not helping our models therefore we will remove them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the farest outlier and see what's in it\n",
    "#df_all_news_processed[df_all_news_processed['Text_TotalWords'] > 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Outliers are equal to {}% of the initial dataset\".format((round(((df_all_news_processed[df_all_news_processed['Text_TotalWords'] > 1400].count()[0])/initial_size)*100, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep records with less than 1500 words\n",
    "df_all_news_processed = df_all_news_processed[df_all_news_processed['Text_TotalWords'] <= 1400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.display_outliers_boxplot(df_all_news_processed, \"Text_TotalWords\", \"Text_TotalWords\", None, \"Records with less of 1400 words\")"
   ]
  },
  {
   "source": [
    "The distribution now seems much better. However we left a small padding in the end of the q4 because of the extra processing that we still have to do. Let's see if that will change before feeding the articles to the models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint: save the dataset as pickle pre-processing\n",
    "df_all_news_processed.to_pickle('data/original_data/pickles/df_all_news_processed_pre_cleaning')"
   ]
  },
  {
   "source": [
    "### Start of the main Text-Processing "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time() \n",
    "#df_all_news_processed = Cleaner.drop_duplicate_values(df_all_news_processed)\n",
    "#df_all_news_processed = Cleaner.remove_new_line(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_non_ascii_chars(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_multiple_spaces(df_all_news_processed)\n",
    "#df_all_news_processed = Cleaner.drop_null_values(df_all_news_processed)\n",
    "#df_all_news_processed = Cleaner.drop_duplicate_values(df_all_news_processed)\n",
    "EDA.process_time(round(time.time()-t1))"
   ]
  },
  {
   "source": [
    "--------------------------------\n",
    "---- where did you find these noisy words? - Explain on another notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "########## N-GRAMS ###########\n",
    "'''articles_split_by_word = NLP.tokenization(df_all_news_processed)\n",
    "n = 3 #Specify for N-Grams\n",
    "occurrences = EDA.find_most_common_n_grams(articles_split_by_word, n)'''"
   ]
  },
  {
   "source": [
    "#sorted(occurrences.items(), key=lambda x: x[1], reverse=True)[:10]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "--------------------------------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Remove most noisy words according to N-Grams notebook analysis (link here)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "t1 = time.time()\n",
    "df_all_news_processed = Cleaner.remove_most_noisy_words(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_multiple_spaces(df_all_news_processed)\n",
    "EDA.process_time(round(time.time()-t1))\n",
    "\n",
    "'''\n",
    "#PROBABLY THE FOLLOWING WILL BE REPLACED FROM THE ABOVE\n",
    "t1 = time.time() \n",
    "df_all_news_processed['Text'] = NLP.to_lower_case(df_all_news_processed, 'Text')\n",
    "df_all_news_processed = Cleaner.remove_most_noisy_words(df_all_news_processed, 'noisy_words.txt')\n",
    "df_all_news_processed = Cleaner.remove_multiple_spaces(df_all_news_processed)\n",
    "EDA.process_time(round(time.time()-t1))'''\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = time.time() \n",
    "df_all_news_processed['Text'] = NLP.to_lower_case(df_all_news_processed, 'Text')\n",
    "articles_split_by_word = NLP.tokenization(df_all_news_processed)\n",
    "df_all_news_processed['Text'] = articles_split_by_word.apply(NLP.remove_stop_words)\n",
    "df_all_news_processed['Text'] = df_all_news_processed['Text'].apply(NLP.rejoin_words) \n",
    "#df_all_news_processed = Cleaner.remove_punctuation(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_single_chars(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_double_chars(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_punctuation_v2(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_multiple_spaces(df_all_news_processed)\n",
    "\n",
    "EDA.process_time(round(time.time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "df_all_news_processed = Cleaner.remove_links(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_dates(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_years(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_months(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_days(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_time(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_punctuation(df_all_news_processed)\n",
    "#second and last iteration of the following 2 cleans\n",
    "df_all_news_processed = Cleaner.remove_single_chars(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_punctuation_v2(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_multiple_spaces(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.drop_duplicate_values(df_all_news_processed)#maybe this should go on top with null values\n",
    "\n",
    "EDA.process_time(round(time.time()-t1))"
   ]
  },
  {
   "source": [
    "After that we are don with cleaning, it's time to move on to balancing our classes. But before we do so, we must remove unnecessary outliers and proceed with clean and correct records.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Outliers removal"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.display_outliers_boxplot(df_all_news_processed, \"Text_TotalWords\", \"Text_TotalWords\", None, \"Articles with number of words after cleaning\")"
   ]
  },
  {
   "source": [
    "#https://help.gooddata.com/doc/en/reporting-and-dashboards/maql-analytical-query-language/maql-expression-reference/aggregation-functions/statistical-functions/predictive-statistical-use-cases/normality-testing-skewness-and-kurtosis\n",
    "#https://pythontic.com/pandas/dataframe-computations/skew\n",
    "pre_outliers_size = len(df_all_news_processed)\n",
    "print(\"----- (Before outliers removal) Skewness value: -----\")\n",
    "EDA.display_outliers_skewness_value(df_all_news_processed, 'Text_TotalWords')\n",
    "\n",
    "df_all_news_processed=EDA.remove_outliers_by_iqr_score(df_all_news_processed)\n",
    "\n",
    "#display_outliers_boxplot(df_all_news_processed, 'Text_TotalWords')\n",
    "print(\"\\n----- (After outliers removal) Skewness value: -----\")\n",
    "EDA.display_outliers_skewness_value(df_all_news_processed, 'Text_TotalWords')\n",
    "\n",
    "print(\"Number of removed outliers: \" + str(pre_outliers_size-len(df_all_news_processed)))\n",
    "total_loss=round((((len(df_all_news_processed)-pre_outliers_size)/pre_outliers_size)*100), 2)\n",
    "print(\"\\n\\n----- Total loss: \" + str(total_loss) +\"% of the initial records -----\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.display_outliers_boxplot(df_all_news_processed, \"Text_TotalWords\", \"Text_TotalWords\", None, \"Our remaining dataset after outliers removal\")"
   ]
  },
  {
   "source": [
    "#checkpoint:\n",
    "#df_all_news_processed.to_pickle('data/original_data/pickles/df_all_news_processed_pre_dataset_distribution')\n",
    "df_all_news_processed = pd.read_pickle(r'data/original_data/pickles/df_all_news_processed_pre_dataset_distribution')\n",
    "pd.options.display.max_colwidth = 500\n",
    "df_all_news_processed"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Way to go: <br/>\n",
    "1) outliers removal (done) <br/>\n",
    "2) Balancing dataset <br/>\n",
    "3) N-Grams + Buzzwords (maybe also per category) <br/>\n",
    "4) NLP (stemming/lemmatize) <br/>\n",
    "5) Data Normalization <br/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Balancing our dataset distribution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First let's see how our current dataset distribution looks like after our cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total records pre-processing: 493432\nTotal subtracted records: -126803\nTotal records post-processing: 366629\nTotal loss: -25.7% of the initial records\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        Category  Count_Pre_Processing  Count_Post_Processing Records_Loss\n",
       "0  entertainment                 50282                  32759      -34.85%\n",
       "1      financial                 47851                  41574      -13.12%\n",
       "2      political                 87157                  36394      -58.24%\n",
       "3          sport                156899                 128715      -17.96%\n",
       "4     technology                 41476                  35306      -14.88%\n",
       "5         travel                 49470                  38183      -22.82%\n",
       "6          world                 60297                  53698      -10.94%"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Category</th>\n      <th>Count_Pre_Processing</th>\n      <th>Count_Post_Processing</th>\n      <th>Records_Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>entertainment</td>\n      <td>50282</td>\n      <td>32759</td>\n      <td>-34.85%</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>financial</td>\n      <td>47851</td>\n      <td>41574</td>\n      <td>-13.12%</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>political</td>\n      <td>87157</td>\n      <td>36394</td>\n      <td>-58.24%</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>sport</td>\n      <td>156899</td>\n      <td>128715</td>\n      <td>-17.96%</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>technology</td>\n      <td>41476</td>\n      <td>35306</td>\n      <td>-14.88%</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>travel</td>\n      <td>49470</td>\n      <td>38183</td>\n      <td>-22.82%</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>world</td>\n      <td>60297</td>\n      <td>53698</td>\n      <td>-10.94%</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "df_all_news_processed['Text_TotalWords'] = EDA.count_words_per_records_opt_2(df_all_news_processed)\n",
    "\n",
    "df_process_stats['Count_Post_Processing'] = EDA.count_records_per_label(df_all_news_processed, \"Count_Post_Processing\")['Count_Post_Processing']\n",
    "\n",
    "df_process_stats = EDA.calculate_loss_percentages(df_process_stats, len(df_all_news_processed), initial_size)\n",
    "df_process_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "hovertext": [
          "Reduced by -34.85%",
          "Reduced by -13.12%",
          "Reduced by -58.24%",
          "Reduced by -17.96%",
          "Reduced by -14.88%",
          "Reduced by -22.82%",
          "Reduced by -10.94%"
         ],
         "marker": {
          "color": "rgb(128, 0, 0)"
         },
         "name": "Pre-Processing",
         "text": [
          50282,
          47851,
          87157,
          156899,
          41476,
          49470,
          60297
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          "entertainment",
          "financial",
          "political",
          "sport",
          "technology",
          "travel",
          "world"
         ],
         "y": [
          50282,
          47851,
          87157,
          156899,
          41476,
          49470,
          60297
         ]
        },
        {
         "hovertext": [
          "Reduced by -34.85%",
          "Reduced by -13.12%",
          "Reduced by -58.24%",
          "Reduced by -17.96%",
          "Reduced by -14.88%",
          "Reduced by -22.82%",
          "Reduced by -10.94%"
         ],
         "marker": {
          "color": "rgb(0, 64, 128)"
         },
         "name": "Post-Processing",
         "text": [
          32759,
          41574,
          36394,
          128715,
          35306,
          38183,
          53698
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          "entertainment",
          "financial",
          "political",
          "sport",
          "technology",
          "travel",
          "world"
         ],
         "y": [
          32759,
          41574,
          36394,
          128715,
          35306,
          38183,
          53698
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "barmode": "group",
        "height": 500,
        "legend": {
         "bgcolor": "rgba(255, 255, 255, 0)",
         "bordercolor": "rgba(255, 255, 255, 0)",
         "x": 0,
         "y": 0.99
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Dataset distribution status post cleaning process"
        },
        "width": 900,
        "xaxis": {
         "title": {
          "text": "Categories"
         }
        },
        "yaxis": {
         "title": {
          "text": "# of Records"
         }
        }
       }
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "EDA.display_stats_distribution_pre_post_processing(df_process_stats, \"Dataset distribution status post cleaning process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopped edw 23/10"
   ]
  },
  {
   "source": [
    "sorted(occurrences.items(), key=lambda x: x[1], reverse=True)"
   ],
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df = pd.DataFrame.from_records(sorted(occurrences.items(), key=lambda x: x[1], reverse=True))\n",
    "df.rename(columns={0: 'words', 1: 'occurs'}, inplace=True)\n",
    "df\n",
    "\n",
    "fig = px.bar(df[:10], x='occurs', y='words')\n",
    "fig.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_news_processed.Category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = EDA.count_records_per_label(df_all_news_processed, \"num_records\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.sort_values(by='num_records', ascending=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "minority_class_name = test['Category'][0]\n",
    "minority_class_number = test['num_records'][0]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(temp_2)\n",
    "#EDA.display_distribution(temp_2, )\n",
    "\n",
    "categories_count = temp_2['Category'].value_counts()\n",
    "EDA.display_distribution(categories_count\n",
    "                        ,\"Distribution of Categories\"\n",
    "                        ,\"Categories\"\n",
    "                        ,\"Number of Articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.count_records_per_label(temp_2, \"num_records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_words = int(temp_2.Text_TotalWords.quantile(0.10))\n",
    "max_words = int(temp_2.Text_TotalWords.quantile(0.90))\n",
    "yolo = temp_2.loc[(temp_2['Text_TotalWords'] >= min_words) & (temp_2['Text_TotalWords'] <= max_words) & (temp_2['Category'] == 'financial')]\n",
    "len(yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_2.Category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = EDA.balance_dataset_distribution(temp_2, 0.10, 0.90)\n",
    "EDA.count_records_per_label(asdf, \"num_records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_count = asdf['Category'].value_counts()\n",
    "EDA.display_distribution(categories_count\n",
    "                        ,\"Distribution of Categories\"\n",
    "                        ,\"Categories\"\n",
    "                        ,\"Number of Articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = temp_2.loc[(temp_2['Text_TotalWords'] <= max_words) & (temp_2['Text_TotalWords'] >= min_words) & (temp_2['Category'] == 'sport')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test.Text.unique())\n",
    "\n",
    "if (len(test)<=minority_class_number2)\n",
    "    test.sample(frac=1)\n",
    "else:\n",
    "    frac_percentage = (minority_class_number2 / len(test))\n",
    "    test.sample(frac=frac_percentage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_processed_smaller.loc[df_processed_smaller['Category'] == 'technology'].sample(2769)\n",
    "print(temp['Text_TotalWords'])\n",
    "test = temp.loc[(temp['Text_TotalWords'] <= max_words) & (temp['Text_TotalWords'] >= min_words)]\n",
    "len(temp.loc[(temp['Text_TotalWords'] <= max_words) & (temp['Text_TotalWords'] >= min_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_count = df_processed_smaller['Category'].value_counts()\n",
    "EDA.display_distribution(categories_count\n",
    "                        ,\"Distribution of Categories\"\n",
    "                        ,\"Categories\"\n",
    "                        ,\"Number of Articles\")"
   ]
  },
  {
   "source": [
    "## NLP & N-Grams / Buzzwords"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemma works. Perhaps use another lemma? https://www.machinelearningplus.com/nlp/lemmatization-examples-python/#wordnetlemmatizer\n",
    "t1 = time.time() \n",
    "articles_split_by_word = NLP.tokenization(df_all_news_processed)\n",
    "df_all_news_processed['Text'] = NLP.stemming(articles_split_by_word)\n",
    "df_all_news_processed['Text'] = df_all_news_processed['Text'].apply(NLP.rejoin_words) \n",
    "\n",
    "EDA.process_time(round(time.time()-t1))\n",
    "#took approx 24 mins on my pc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### NAME ENTITY RECOGNITION - START #####   \n",
    "###########################################\n",
    "'''\n",
    "MY COMMENTS:\n",
    "Entity Recognition package of NLTK does not work fully efficiently. It recognises false names.\n",
    "More importantly is super slow for 10 articles and it would require way better hardware capabilities to run for\n",
    "my large corpus 300k+ articles.\n",
    "\n",
    "todo: In the end I will exclude the jar files from the libraries folder and I will just mention \n",
    "it as observation to my final report.\n",
    "'''\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "\n",
    "import os\n",
    "\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_201/bin/java.exe\"   \n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "\n",
    "# Add the jar and model via their path (instead of setting environment variables):\n",
    "jar = 'libraries/stanford-postagger-full-2020-08-06/stanford-postagger.jar'\n",
    "model = 'libraries/stanford-postagger-full-2020-08-06/models/english-left3words-distsim.tagger'\n",
    "pos_tagger = StanfordPOSTagger(model, jar, encoding='utf8')\n",
    "\n",
    "def find_names(row):\n",
    "    for sent in nltk.sent_tokenize(row):\n",
    "        tokens = nltk.tokenize.word_tokenize(sent)\n",
    "        tags = pos_tagger.tag(tokens)\n",
    "        print(tags)\n",
    "        for tag in tags:\n",
    "            if tag[1] in [\"NNP\"]:\n",
    "                print(tag)\n",
    "        print(\"-----\")\n",
    "######################################            \n",
    "\n",
    "df_test=df_all_news_processed[:5].copy()\n",
    "df_test['Text'] = df_test['Text'].str.upper()\n",
    "df_test['Text'].apply(find_names) \n",
    "\n",
    "##### NAME ENTITY RECOGNITION - END #####     \n",
    "######################################### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.display_buzzwords(list(df_processed_smaller['Text'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all_news_processed['clean_data'] = df_all_news_processed['clean_data'].apply(NLP.rejoin_words) \n",
    "df_all_news_processed.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test = df_all_news_processed[:100].copy()\n",
    "test = df_all_news_processed.copy()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test.replace(',',' ', regex=True, inplace=True)\n",
    "#YOU NEED TO PASS THE DATA AS A LIST WITHOUT COMMA WITHIN THE TEXT.\n",
    "EDA.display_buzzwords(list(df_all_news_processed['Text'].values))\n",
    "'''\n",
    "buzzwords bechmark on my MAC: 16 GB 1600 MHz DDR3 / 2,5 GHz Quad-Core Intel Core i7\n",
    "    / articles: 324.965 \n",
    "    / total number of words: 150.614.863 (with stopwords)\n",
    "    / process time: 2.631 seconds (around 44 minutes)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_news_processed.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all_news_processed.groupby(['Category']).mean()\n",
    "asdf = df_all_news_processed.groupby('Category')['Text_TotalWords'].mean()\n",
    "EDA.display_distribution(asdf\n",
    "                        ,\"Mean Number of words per Article by Categories\"\n",
    "                        ,\"Categories\"\n",
    "                        ,\"Mean N# of words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categories_count = df_all_news_processed['Category'].value_counts()\n",
    "EDA.display_distribution(categories_count\n",
    "                        ,\"Distribution of Categories\"\n",
    "                        ,\"Categories\"\n",
    "                        ,\"Number of Articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all_news_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''''''''''''''''' ___MAIN PIPELINE___ '''''''''''''''''''\n",
    "\n",
    "df_all_news = pd.read_pickle(r'data/original_data/pickles/all_news')\n",
    "\n",
    "### Cleaning part\n",
    "df_all_news['Text_TotalWords'] = EDA.count_words_per_records_opt_2(df_all_news)\n",
    "\n",
    "df_all_news.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_all_news_processed = df_all_news.copy()\n",
    "#remove outliers - first phase\n",
    "df_all_news_processed = df_all_news_processed[df_all_news_processed['Text_TotalWords'] > 10] \n",
    "df_all_news_processed = df_all_news_processed[df_all_news_processed['Text_TotalWords'] < 3000] \n",
    "\n",
    "df_all_news_processed = Cleaner.drop_duplicate_values(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_new_line(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_non_ascii_chars(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_multiple_spaces(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.drop_null_values(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.drop_duplicate_values(df_all_news_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_news_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N-grams function (done)\n",
    "#buzzwords function (done)\n",
    "#sentence encoder (too large texts to visaulise interactively)\n",
    "\n",
    "\n",
    "df_all_news_processed['Text'] = NLP.to_lower_case(df_all_news_processed, 'Text')\n",
    "df_all_news_processed = Cleaner.remove_most_noisy_words(df_all_news_processed, 'noisy_words.txt')\n",
    "df_all_news_processed = Cleaner.remove_multiple_spaces(df_all_news_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "articles_split_by_word = NLP.tokenization(df_all_news_processed)\n",
    "df_all_news_processed['Text'] = articles_split_by_word.apply(NLP.remove_stop_words)\n",
    "df_all_news_processed['Text'] = df_all_news_processed['Text'].apply(NLP.rejoin_words) \n",
    "print(\"reached here\")\n",
    "df_all_news_processed = Cleaner.remove_punctuation(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_single_chars(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_multiple_spaces(df_all_news_processed)\n",
    "#CHECKPOINT: Print N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_news_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_news_processed = Cleaner.remove_links(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_dates(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_years(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_months(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_days(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_time(df_all_news_processed)\n",
    "df_all_news_processed = Cleaner.remove_multiple_spaces(df_all_news_processed)\n",
    "#CHECKPOINT: Print N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize (if applicable)\n",
    "#remove outliers\n",
    "df_all_news['Text_TotalWords'] = EDA.count_words_per_records_opt_2(df_all_news_processed)\n",
    "df_all_news_processed = df_all_news_processed[df_all_news_processed['Text_TotalWords'] > 10]\n",
    "df_all_news_processed = df_all_news_processed[df_all_news_processed['Text_TotalWords'] < 600]\n",
    "\n",
    "#N-grams function (done)\n",
    "#buzzwords function (done)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = time.time() \n",
    "\n",
    "articles_split_by_word = NLP.tokenization(df_all_news_processed)\n",
    "df_all_news_processed['Text'] = NLP.stemming(articles_split_by_word)\n",
    "df_all_news_processed['Text'] = df_all_news_processed['Text'].apply(NLP.rejoin_words) \n",
    "\n",
    "t2 = time.time() \n",
    "print(\"Process completed.\\nTime taken:\") \n",
    "print(\"{:.2f}\".format(round(t2-t1, 2))+\" seconds.\")\n",
    "\n",
    "#took 1446 seconds(approx 24 mins) on my pc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_news_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all_news_processed.to_pickle('data/original_data/pickles/df_all_news_processed_stemmed')"
   ]
  },
  {
   "source": [
    "### Data Normalization & Feature Engineering  (normalize, tf-idf vectors)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = time.time() \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7)\n",
    "X = tfidfconverter.fit_transform(df_all_news_processed['Text']).toarray()\n",
    "\n",
    "t2 = time.time() \n",
    "print(\"Process completed.\\nTime taken:\") \n",
    "print(\"{:.2f}\".format(round(t2-t1, 2))+\" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = time.time() \n",
    "\n",
    "y = df_all_news_processed['Category']\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "t2 = time.time() \n",
    "print(\"Process completed.\\nTime taken:\") \n",
    "print(\"{:.2f}\".format(round(t2-t1, 2))+\" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "source": [
    "### Model selection (SVM + kNN + naiveBayes + logistic regression on textual data?)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "pd.options.display.max_colwidth = 500\n",
    "df_all_news_processed.loc[df_all_news_processed['Category'] == 'travel']"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''######################## Find N-Grams ########################\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "#This function is called from find_most_common_n_grams()\n",
    "def find_ngrams(input_list, n, n_common_words):\n",
    "    return list(zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "\n",
    "########### [Input] ###########\n",
    "########## df: The DataFrame with the data that would like to process\n",
    "########## n: The number of N-grams \n",
    "########## n_top_ngrams: The number the X most common N-grams\n",
    "########## Returns: A list with the X most common N-grams and their occurrences \n",
    "def find_most_common_n_grams(df, n, n_top_ngrams):\n",
    "    \n",
    "    n_grams_title = str(n) +'_grams'\n",
    "    \n",
    "    df[n_grams_title] = df['Text'].map(lambda x: find_ngrams(x.split(\" \"), n, 10))\n",
    "    \n",
    "    ngrams = df[n_grams_title].tolist()\n",
    "    ngrams = list(chain(*ngrams))\n",
    "\n",
    "    #TODO this: should have x,y,z as the n increases \n",
    "    # e.g. [(x.lower(), y.lower(), z.lower()) for x,y,z in ngrams] #--> For 3-grams etc.\n",
    "    #ngrams = [(x.lower(), y.lower()) for x,y in ngrams]\n",
    "\n",
    "    ngram_counts = Counter(ngrams)\n",
    "    return ngram_counts.most_common(n_top_ngrams)\n",
    "\n",
    "#4 stands for: fourgrams. Change to any n value\n",
    "#20 stands for: display the top 20 n-grams\n",
    "testing = find_most_common_n_grams(df_all_news_processed, 4, 20)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''''''''''''''''''' -----NOT USED------''''''''''''''''''''''''''\n",
    "\n",
    "#THIS CANNOT BE DONE BECAUSE WE DONT WANT TO REPLACE THE OUTLIERS WITH THE MEAN VALUE\n",
    "#BECAUSE OF THE HIGH NUMBERS OF THE OUTLIERS, WE WANT TO REMOVE THEM COMPLETELY.\n",
    "#PLUS THIS IS ABOUT NUMBER OF WORDS PER ARTICLE. WE CANT REPLACE ARTICLES WITH OTHER ARTICLES\n",
    "#BECUASE THAT WOULD MESS UP OUR DATASET.\n",
    "def remove_outliers_quantile_percentages(df, column_name, low_percent, high_percent):\n",
    "    low_value = df[column_name].quantile(low_percent)\n",
    "    high_value = df[column_name].quantile(high_percent)\n",
    "    df[column_name] = np.where(df[column_name] <low_value, low_value,df[column_name])\n",
    "    df[column_name] = np.where(df[column_name] >high_value, high_value,df[column_name])\n",
    "    return df[column_name]\n",
    "    \n",
    "display_outliers_skewness_value(test, 'Text_TotalWords')\n",
    "test['Text_TotalWords'] = remove_outliers_quantile_percentages(test, 'Text_TotalWords', 0.10, 0.90)\n",
    "display_outliers_skewness_value(test, 'Text_TotalWords')\n",
    "'''''''''''''''''''' -----NOT USED------''''''''''''''''''''''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "######## SERIAL CLEANING STEPS ######\n",
    "Remove new line\n",
    "Remove non-ascii chars\n",
    "Remove multiple spaces\n",
    "Remove null values\n",
    "Remove duplicate values\n",
    "Remove noisy pre-defined words (by list) / n-grams (to-do)\n",
    "Remove punctuations\n",
    "to lower(NLP)\n",
    "Remove single chars\n",
    "Remove links\n",
    "Remove years\n",
    "Remove months (to do)\n",
    "Remove days\n",
    "Remove time\n",
    "Remove multiple spaces\n",
    "Remove stopwords(NLP)\n",
    "\n",
    "Remove outliers(EDA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}